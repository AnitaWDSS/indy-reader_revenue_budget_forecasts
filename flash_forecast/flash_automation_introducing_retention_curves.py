# -*- coding: utf-8 -*-
"""Flash Automation - Introducing Retention Curves

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OTA3SpewbkturPo2ijGT6CXdb4PL0mWA
"""

from dateutil.relativedelta import relativedelta
import pandas as pd
from google.cloud import bigquery
import random
import numpy as np
from datetime import date

from google.colab import auth
auth.authenticate_user()   # Authenticate your Google account

"""# Initial Import

Below three key changes are made to the raw transaction data


1.   New `flash_tenure` dimension is created to consider users on their trial end date days as tenured (for amortisation purposes)
2.   To optimise script, only transactions for the last 12 months is taken (also for amortisation purposes)
3. Only transactions categorised as "completed" are considered
"""

# Uploading transactions_enhanced dataset

# Note - is_trialist definition considers a user as trialist on the day of their
# conversion to tenure
# CASE
# WHEN DATE(subscription_trial_end_date) <CURRENT_DATE()
# THEN 'tenured'
# WHEN subscription_trial_end_date IS NULL
# THEN 'tenured'
# ELSE 'trialist'
# END AS is_trialist

query = """
SELECT
    piano_uid,
    customer_type,
    package_type,
    is_trialist,
    CASE WHEN DATE(subscription_trial_end_date) <= date OR subscription_trial_end_date IS NULL OR trial_cadence = 'No trial'
    THEN 'tenured' ELSE 'trialist' END AS flash_tenure, # Need this exclusively for amortisation - slightly differs vs is_trialist as here, at trial_end_date one is considered tenured
    start_date,
    tax_type,
    transaction_status,
    term_name,
    term_id,
    subscription_id,
    local_price,
    currency,
    price,
    expires,
    tax,
    tax_base,
    tax_rate,
    tax_country,
    term_cadence,
    geo,
    term_price,
    trial_price,
    trial_cadence,
    DATE_DIFF(CURRENT_DATE(), DATE(start_date), MONTH) AS month_index,
    year_month,
    date
FROM `indy-eng.reader_revenue_dataform.transactions_log_enhanced`
WHERE DATE_TRUNC(date, MONTH) >= DATE_TRUNC(date, MONTH) - INTERVAL 12 MONTH # Only include last 12 months as money for this month dependent only on the last year of transactions (as annual subs are amortised over 12 months)
AND transaction_status = 'completed' # Removes any pending, cancelled or refunded transanctions (note, refunds already have their own separate negative "completed" transaction)
"""

client = bigquery.Client(project='indy-eng')
query_job = client.query(query)
transactions_df = query_job.to_dataframe()

transactions_df.term_cadence.unique()

# Checking max_date passed (Day before current date)
transactions_df.date.max().day

"""## Summed Local Price

Creates the summed local prices by:
* Subsetting the initial SQL query
* Calculates the summed local price summing local prices over all splits
* Cohorts divided by the year-month a subscription was taken out
"""

grouping_columns = [
    "piano_uid",
    "term_name",
    "term_id",
    "currency",
    "term_cadence",
    "geo",
    "customer_type",
    "package_type",
    "is_trialist",
    "flash_tenure",
    "trial_price",
    "term_price",
    "tax_type",
    "month_index",
    "trial_cadence",
    "year_month"
]

subset_columns = grouping_columns + ["local_price"]

grouped_transactions_df_subset = transactions_df[subset_columns]
grouped_transactions_df = grouped_transactions_df_subset.groupby(grouping_columns)["local_price"].sum().reset_index(name="summed_local_price")
grouped_transactions_df

refund_amount=pd.DataFrame(grouped_transactions_df[grouped_transactions_df.summed_local_price<=0]['summed_local_price'].unique(), columns=['summed_local_price']).sort_values('summed_local_price', ascending=False).head()

print(refund_amount)

"""## User count

Calculates the number of users billed within each split
"""

# removes piano_uid from summed_local_price grouping
users_grouping_columns = [
    "term_name",
    "term_id",
    "currency",
    "term_cadence",
    "geo",
    "customer_type",
    "package_type",
    "is_trialist",
    "flash_tenure",
    "trial_price",
    "term_price",
    "tax_type",
    "month_index",
    "trial_cadence",
    "year_month",
    "summed_local_price"
]
users_count_df = (
    grouped_transactions_df.groupby(users_grouping_columns)["piano_uid"]
    .count()
    .reset_index(name="user_count")
)

checking_subs = users_count_df.loc[users_count_df.customer_type == 'Subscription', ['package_type','term_name', 'is_trialist','flash_tenure','summed_local_price','term_cadence','trial_cadence','term_price','trial_price','month_index', 'year_month']].sort_values(['month_index'])
checking_subs

"""## User Base & Amortises Revenue

Once we have both the number users and the amounts billed per split, we calculate:
1. The user base still active per split. Unless on a monthly term, this includes all the people that have had a billing in a period equal to their term_cadence. E.g.: For annual subs, whether they have had a billing in the last year.
2. The amortised revenue in a month: finance splits a user's billed amount by the period of time they are active for
"""

# Define relevant cadence needed to amortise transaction
users_count_df["relevant_cadence"] = np.where(
    users_count_df["flash_tenure"] == "trialist",
    users_count_df["trial_cadence"],
    users_count_df["term_cadence"]
)

# Turn their cadence into number of months they are active for (for anything smaller than a month, we limit it to the moth in which a payment was made)
cadence_map = {
    "day": 1,
    "week": 1,
    "2 week":1,
    "month": 1,
    "quarter": 3,
    "6 month":  6,
    "year": 12,
    "3 year": 36,
}

users_count_df["relevant_cadence_months"] =  users_count_df["relevant_cadence"].map(cadence_map)

# Renaming for clarity
clean_transactions_df = users_count_df

# Checking that cadence map addresses all terms
clean_transactions_df.relevant_cadence_months.unique()
clean_transactions_df[clean_transactions_df.relevant_cadence_months.isna()]

# Ensure transactions are only amortised throughout the subscription cadence if they're payments - refunds do not get amortised
clean_transactions_df["amortised_summed_local_price"] = np.where(
    clean_transactions_df["summed_local_price"]>=0,
    clean_transactions_df["summed_local_price"] / clean_transactions_df["relevant_cadence_months"],
    clean_transactions_df["summed_local_price"]
)
# Ensure year_month is of type date_time for calculations
clean_transactions_df["year_month"] = pd.to_datetime(clean_transactions_df["year_month"])

grouping_columns = [
    "term_name",
    "term_id",
    "term_cadence",
    "geo",
    "customer_type",
    "package_type",
    "is_trialist",
    "flash_tenure",
    "term_price",
    "currency",
    "tax_type",
    "month_index",
    "trial_cadence",
    'trial_price',
    "summed_local_price",
    "relevant_cadence",
    "amortised_summed_local_price",
    "relevant_cadence_months", # Needs to be last column
]
# Defining function to calculate user_base
def calculate_user_base(group):

    cadence_month = group.name[-1]

    min_date = group["year_month"].min()
    max_date = group["year_month"].max() + (relativedelta(months=max(cadence_month,1)-1))

    # Generate complete date range

    all_dates = pd.date_range(min_date, max_date, freq="MS")

    # Reindex to fill missing months
    group = group.set_index("year_month").reindex(all_dates, fill_value=0)

    # Calculates rolling sum of user_count across the cadence_months to obtain the active user base
    group["user_base"] = group["user_count"].rolling(cadence_month, min_periods=1).sum()

    group = group.reset_index(names="year_month")
    return group

# Executes function
amortised_transactions_df = (
    clean_transactions_df.groupby(grouping_columns)
    .apply(calculate_user_base, include_groups=False)
).reset_index()

# Calculated amortised revenue
amortised_transactions_df["amortised_revenue"] = np.where(
    amortised_transactions_df["amortised_summed_local_price"]>=0,
    amortised_transactions_df["user_base"] * amortised_transactions_df["amortised_summed_local_price"],
    amortised_transactions_df["user_count"] * amortised_transactions_df["amortised_summed_local_price"]
)

## Checking user base & amortisation
amortised_transactions_df[(amortised_transactions_df.customer_type == 'Subscription') & (amortised_transactions_df.flash_tenure == 'tenured')]

"""## Currency Conversion"""

# Currency Exchange
# If no available currency conversion exists (for a date in the future), select the latest available
query = """
  WITH base_data AS (
  SELECT
    Date,
    Currency_Code,
    MAX(GBP_Conversion) AS GBP_Conversion
  FROM `indy-eng.partners.currency_matchtable`
  GROUP BY ALL
),
currency_codes AS (
  SELECT DISTINCT Currency_Code
  FROM `indy-eng.partners.currency_matchtable`
),
date_range AS (
  SELECT date_val AS Date
  FROM UNNEST(
    GENERATE_DATE_ARRAY(
      (SELECT MIN(Date) FROM base_data),
      DATE_ADD(CURRENT_DATE(), INTERVAL 12 MONTH),
      INTERVAL 1 DAY
    )
  ) AS date_val
),
complete_calendar AS (
  SELECT
    d.Date,
    c.Currency_Code
  FROM date_range d
  CROSS JOIN currency_codes c
)
SELECT
  cc.Date,
  cc.Currency_Code,
  LAST_VALUE(bd.GBP_Conversion IGNORE NULLS) OVER (
    PARTITION BY cc.Currency_Code
    ORDER BY cc.Date
    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
  ) AS GBP_Conversion
FROM complete_calendar cc
LEFT JOIN base_data bd
  ON cc.Date = bd.Date
  AND cc.Currency_Code = bd.Currency_Code
ORDER BY cc.Currency_Code, cc.Date


"""

query_job = client.query(query)
currency_conversion_df = query_job.to_dataframe()
currency_conversion_df["Date"] = pd.to_datetime(currency_conversion_df["Date"])

GBP_amortised_transactions_df = amortised_transactions_df.merge(currency_conversion_df, how="left", left_on=["year_month", "currency"], right_on=["Date", "Currency_Code"]).drop(columns=["Date", "Currency_Code"])
GBP_amortised_transactions_df["gbp_amortised_revenue"] = GBP_amortised_transactions_df["amortised_revenue"] * GBP_amortised_transactions_df["GBP_Conversion"]

checking_currency = GBP_amortised_transactions_df.loc[GBP_amortised_transactions_df.customer_type == 'Subscription', ['package_type','term_name', 'is_trialist','flash_tenure','term_cadence','trial_cadence','term_price','trial_price','month_index', 'year_month','amortised_revenue','gbp_amortised_revenue']].sort_values(['month_index'])
checking_currency

"""# Forecasting EOM Performance (With Retention Curves)

1. Identify number of users due to renew in the current month
2. Upload rentention curves
3. Calculate MoM retention likelihood difference for all month_indexes found among users due to renew
4. Use said difference to calculate expected EOM number of billed users
5. Calculate expected amount of amortised revenue
"""

#  Pulling up users due to renew this month
query = """
SELECT
  piano_uid,
  piano_status,
  term_id,
  subscription_id,
  DATE(start_date) AS start_date,
  subscription_grace_period_start_date,
  subscription_trial_end_date,
  next_billing_date,
  month_index,
  geo,
  term_name,
  trial_cadence,
  term_cadence,
  term_price,
  trial_price,
  is_trialist,
  customer_type,
  package_type
FROM
  `indy-eng.reader_revenue_dataform.subscription_details_enhanced`
WHERE
  (DATE_TRUNC(next_billing_date, MONTH)=DATE_TRUNC(CURRENT_DATE(), MONTH)
  # Needed the below as transactions log oly updated once a day in the morning
  OR last_billing_date = CURRENT_DATE())
  AND package_type != 'App Subscription'
"""
client = bigquery.Client(project='indy-eng')
query_job = client.query(query)
renewal_due_df = query_job.to_dataframe()

# Uploading retention curve values
from google.colab import drive

drive.mount('/content/drive')

folder_path = '/content/drive/MyDrive/DATA PROJECTS/Subscriptions Forecasting/'
file_path = folder_path + 'retention_curves.csv'

retention_curves_df = pd.read_csv(file_path)

print(retention_curves_df.columns) # Print column names

retention_curves_df.drop(['Unnamed: 0'], axis=1, inplace=True)

retention_curves_df

#  Calculate number of users per cohort and renewal, month
renewal_due_df['next_billing_date'] = pd.to_datetime(renewal_due_df['next_billing_date'])
renewal_due_df['renewal_month']= renewal_due_df['next_billing_date'].dt.to_period('M').dt.to_timestamp()

renewal_due_count= renewal_due_df.groupby(['geo','trial_cadence','trial_price','term_cadence','term_price','term_type','renewal_month','month_index'])['piano_uid'].count().reset_index(name='piano_uid_count')

renewal_due_count[['geo','term_type', 'term_cadence','trial_cadence', 'term_price', "trial_price",'piano_uid_count']].sort_values(['piano_uid_count'], ascending=False)

renewal_due_count['next_month_index'] = renewal_due_count['month_index'] + 1
renewal_due_count['local_price'] = renewal_due_count['term_price'].str.replace(r'[^\d.,\-]', '', regex=True)
renewal_due_count['local_price'] = renewal_due_count['local_price'].astype(float)

forecasted_renewals=pd.merge(renewal_due_count, retention_curves_df[['month_index','geo','cadence','term_type','average_retention_rates_smoothed']], how='left', left_on=['next_month_index', 'geo','term_cadence', 'term_type'], right_on=['month_index', 'geo','cadence','term_type'] ).rename(columns={'average_retention_rates_smoothed': 'next_month_retention_rate', 'month_index_x': 'current_month_index'}).drop(columns=['month_index_y'])
forecasted_renewals=pd.merge(forecasted_renewals, retention_curves_df[['month_index','geo','cadence','term_type','average_retention_rates_smoothed']], how='left', left_on=['current_month_index', 'geo','term_cadence', 'term_type'], right_on=['month_index', 'geo','cadence','term_type'] ).rename(columns={'average_retention_rates_smoothed': 'current_retention_rate','cadence_x':'cadence'}).drop(columns=['month_index','cadence_y'])

forecasted_renewals['forecasted_renewals'] = round(forecasted_renewals['piano_uid_count'] * forecasted_renewals['next_month_retention_rate']/ forecasted_renewals['current_retention_rate'],2)

# Used to quickly calculate how much forecasted return
forecasted_renewals.groupby(['geo','term_type','trial_cadence','term_cadence','trial_price','term_price'])['forecasted_renewals'].sum().reset_index(name='forecasted_renewals').sort_values('forecasted_renewals', ascending=False)

final_current_df=amortised_users_grouped_transactions_converted_currency_df

final_current_df.trial_price

forecasted_renewals.trial_price

forecasted_and_current_revenue=pd.merge(
    final_current_df,
    forecasted_renewals[['geo', 'trial_cadence', 'term_cadence', 'term_price','term_type','forecasted_renewals','local_price','renewal_month','current_month_index', 'trial_price']],
    how='left',
    left_on=['geo', 'trial_cadence','trial_price', 'term_cadence', 'term_price','term_type','summed_local_price','year_month','month_index'],
    right_on=['geo', 'trial_cadence','trial_price','term_cadence', 'term_price','term_type', 'local_price','renewal_month', 'current_month_index'])

forecasted_and_current_revenue[forecasted_and_current_revenue['forecasted_renewals'].isna()== False]

import calendar as cal

current_date = pd.Timestamp.now()
max_days_in_month = cal.monthrange(current_date.year, current_date.month)[1]
num_past_days = transactions_df.date.max().day


conditions=[
    ((forecasted_and_current_revenue['month_index'] == 0)  |
    (forecasted_and_current_revenue['summed_local_price']<= 0) |
    (forecasted_and_current_revenue['term_type']=='Single Donation')) &
    (forecasted_and_current_revenue['year_month'].dt.to_period('M') == current_date.to_period('M')) ,
    (forecasted_and_current_revenue['forecasted_renewals'].isna() == True)
]

options = [
    (forecasted_and_current_revenue['user_count']/num_past_days*(max_days_in_month-num_past_days)),
    (0)
]

forecasted_and_current_revenue['forecasted_renewals']=np.select(conditions, options, default=forecasted_and_current_revenue['forecasted_renewals'])



forecasted_and_current_revenue.local_price.unique()

forecasted_and_current_revenue.head()

forecasted_and_current_revenue['forecasted_user_count']=forecasted_and_current_revenue['user_count']+forecasted_and_current_revenue['forecasted_renewals']
forecasted_and_current_revenue['forecasted_user_base']=forecasted_and_current_revenue['user_base']+forecasted_and_current_revenue['forecasted_renewals']
forecasted_and_current_revenue['forecasted_amortised_revenue']=np.where(
    forecasted_and_current_revenue['amortised_summed_local_price']>=0,
    forecasted_and_current_revenue['forecasted_user_base']*forecasted_and_current_revenue['amortised_summed_local_price'],
    forecasted_and_current_revenue['forecasted_user_count']*forecasted_and_current_revenue['amortised_summed_local_price']
)

forecasted_and_current_revenue['forecasted_gbp_amortised_revenue']=forecasted_and_current_revenue['forecasted_amortised_revenue']*forecasted_and_current_revenue['GBP_Conversion']

forecasted_and_current_revenue['forecasted_added_gbp_revenue_amortised']= forecasted_and_current_revenue['forecasted_renewals']* forecasted_and_current_revenue['amortised_summed_local_price']*forecasted_and_current_revenue['GBP_Conversion']

forecasted_revenue_this_month = forecasted_and_current_revenue[
    (forecasted_and_current_revenue['year_month'].dt.year == current_date.year) &
    (forecasted_and_current_revenue['year_month'].dt.month == current_date.month)
]

forecasted_revenue_this_month.groupby(['geo', 'is_donation'])['forecasted_gbp_amortised_revenue'].sum()

forecasted_and_current_revenue[
    (forecasted_and_current_revenue['year_month'].dt.year == current_date.year) &
    (forecasted_and_current_revenue['year_month'].dt.month == current_date.month) &
    (forecasted_and_current_revenue['summed_local_price']>=0)]['forecasted_added_gbp_revenue_amortised'].sum()

forecasted_and_current_revenue[
    (forecasted_and_current_revenue['year_month'].dt.year == current_date.year) &
    (forecasted_and_current_revenue['year_month'].dt.month == current_date.month)]['gbp_amortised_revenue'].sum()

is_donation_list=['Recurring Donations','Single Donation']
forecasted_revenue_this_month['is_donation'] = forecasted_revenue_this_month['term_type'].isin(is_donation_list)
checking_values=forecasted_revenue_this_month.groupby(['geo','is_donation', 'trial_price', 'trial_cadence', 'term_price', 'term_cadence', 'tenure'])['forecasted_user_base'].sum().sort_values(ascending=False)

checking_values= checking_values.reset_index()
checking_values[checking_values['tenure'] == 'tenured']

forecasted_revenue_this_month['is_subscriber'] = forecasted_revenue_this_month.term_type.isin(['Current Annual', 'Current Monthly','No Category', 'Student', 'Legacy Monthly', 'Legacy Annual', 'COP' ])

forecasted_revenue_this_month[ (forecasted_revenue_this_month['is_subscriber'])]['forecasted_user_base'].sum()

forecasted_revenue_this_month[(forecasted_revenue_this_month['tenure'] == 'trialist') & (forecasted_revenue_this_month['is_subscriber'])]['forecasted_user_base'].sum()

forecasted_revenue_this_month[forecasted_revenue_this_month['summed_local_price'] <=0]['forecasted_amortised_revenue'].sum()

# Adding in cases when
1. Added expected acquisition (for cases when tenure= trialist then calculate user log as in transactions log)
2. Add expected refunds ( for cases when summed local price <= 0 then calculate user log as in transactions log)
3. Add expected single donations (for case where term_type is single donation)

"""##  Revenue Check"""

a = np.sum(amortised_users_grouped_transactions_converted_currency_df["summed_local_price"] * amortised_users_grouped_transactions_converted_currency_df["user_count"])
b = np.sum(amortised_users_grouped_transactions_converted_currency_df["amortised_summed_local_price"] * amortised_users_grouped_transactions_converted_currency_df["user_base"])
c = np.sum(amortised_users_grouped_transactions_converted_currency_df["amortised_revenue"])

if np.isclose(a, b) and np.isclose(b, c):
    print("Revenue test passed")
else:
    print(a, b, c)

    raise Exception("Revenue test failed")

"""## Sample Testing"""

random_term_id = random.choice(amortised_users_grouped_transactions_converted_currency_df["term_id"].unique())

test_df = amortised_users_grouped_transactions_converted_currency_df[
    (amortised_users_grouped_transactions_converted_currency_df["term_id"] == random_term_id)
].sort_values("YearMonth")

random_tenure = random.choice(test_df["tenure"].unique())

test_df[
].sort_values("YearMonth")

random_summed_local_price = random.choice(test_df["summed_local_price"].unique())

test_df = test_df[
    (test_df["summed_local_price"] == random_summed_local_price)
].sort_values("YearMonth")

a = np.sum(test_df["summed_local_price"] * test_df["user_count"])
b = np.sum(test_df["amortised_summed_local_price"] * test_df["user_base"])
c = np.sum(test_df["amortised_revenue"])

revenue_test = np.isclose(a, b) and np.isclose(b, c)

if not revenue_test:
    print(a, b, c)
    raise Exception("Revenue test failed")


test_df

# TO DO

# Trialist usng sub_details / Negative trial amount

"""# Bigquery Import"""

# job_config = bigquery.LoadJobConfig()
# job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
# load_job = client.load_table_from_dataframe(grouped_transactions_df,'indy-eng.users_v2.flash_automation', job_config=job_config)
# load_job.result()

"""# Domo Import"""

# !pip install pydomo
# from pydomo import Domo